{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqoCqJxrLxEI",
        "outputId": "4229a0e5-cf5e-40ca-8ab9-699c4ea4ec1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Collecting gTTs\n",
            "  Downloading gTTS-2.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gTTs) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTs) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTs) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gTTs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTs) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTs) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=f2558f4b97a575464d3774143792ff9c2cf9e1406b43a19acd420d7e9ab0b4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: SpeechRecognition, sklearn, gTTs\n",
            "Successfully installed SpeechRecognition-3.8.1 gTTs-2.2.4 sklearn-0.0\n"
          ]
        }
      ],
      "source": [
        "pip install SpeechRecognition numpy gTTs sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mpg123"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9BstpPLLz49",
        "outputId": "42d022c2-4615-49f4-bf7c-663b6302fc5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpg123\n",
            "  Downloading mpg123-0.4.tar.gz (2.2 kB)\n",
            "Building wheels for collected packages: mpg123\n",
            "  Building wheel for mpg123 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpg123: filename=mpg123-0.4-py3-none-any.whl size=2862 sha256=8a756bbacec0fd5635dab7317cc5d1abbbf1d5f18ac3a176cb36012b0bd4ae94\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/d1/b9/a36a8c0a5b6fd0839c65a486511ac294d0c46ef4901fd6af38\n",
            "Successfully built mpg123\n",
            "Installing collected packages: mpg123\n",
            "Successfully installed mpg123-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBZlAp0nL5yf",
        "outputId": "fc7e1549-f5d5-4d31-892c-e830d723b423"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.12.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (PEP 517) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\n",
            "Failed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio which use PEP 517 and cannot be installed directly\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "from gtts import gTTS\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr \n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#for downloading package files can be commented after First run\n",
        "nltk.download('popular', quiet=True)\n",
        "nltk.download('nps_chat',quiet=True)\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkP_94N0MDlG",
        "outputId": "baff03ce-77a7-47b5-b61f-09e4d447e4b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
        "# To Recognise input type as QUES. \n",
        "def dialogue_act_features(post):\n",
        "    features = {}\n",
        "    for word in nltk.word_tokenize(post):\n",
        "        features['contains({})'.format(word.lower())] = True\n",
        "    return features\n",
        "featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\n",
        "size = int(len(featuresets) * 0.1)\n",
        "train_set, test_set = featuresets[size:], featuresets[:size]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "KueCHjCwTsEW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword Matching\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "2z67rVwYTzF2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading in the input_corpus\n",
        "with open('/content/intro_join.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
        "    raw = fin.read().lower()\n",
        "#TOkenisation\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
        "# Preprocessing\n",
        "lemmer = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "eUbJcQrST3O8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \n",
        "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk)) \n",
        "def prYellow(skk): print(\"\\033[93m {}\\033[00m\" .format(skk)) \n",
        "def prLightPurple(skk): print(\"\\033[94m {}\\033[00m\" .format(skk)) \n",
        "def prPurple(skk): print(\"\\033[95m {}\\033[00m\" .format(skk)) \n",
        "def prCyan(skk): print(\"\\033[96m {}\\033[00m\" .format(skk)) \n",
        "def prLightGray(skk): print(\"\\033[97m {}\\033[00m\" .format(skk)) \n",
        "def prBlack(skk): print(\"\\033[98m {}\\033[00m\" .format(skk))"
      ],
      "metadata": {
        "id": "xrsUGZSST8_h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "metadata": {
        "id": "cAftYL2FUyFo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recording voice input using microphone \n",
        "file = \"file.mp3\"\n",
        "flag=True\n",
        "fst=\"My name is Jarvis. I will answer your queries about Science. If you want to exit, say Bye\"\n",
        "tts = gTTS(text=fst,lang=\"en\" ,tld=\"com\")\n",
        "tts.save(file)\n",
        "os.system(\"mpg123 \" + file )\n",
        "r = sr.Recognizer()\n",
        "prYellow(fst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTUb0bvPU2pS",
        "outputId": "91abd559-9deb-4dae-a3ec-329b15604f4f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93m My name is Jarvis. I will answer your queries about Science. If you want to exit, say Bye\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pipwin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8tosxTkWNg1",
        "outputId": "831bfaff-5e18-4091-c984-2e31ebf9ff03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pipwin\n",
            "  Downloading pipwin-0.5.2.tar.gz (7.9 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pipwin) (2.23.0)\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pipwin) (1.15.0)\n",
            "Collecting beautifulsoup4>=4.9.0\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting js2py\n",
            "  Downloading Js2Py-0.71-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 32.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pipwin) (21.3)\n",
            "Collecting pySmartDL>=1.3.1\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting pyjsparser>=2.5.1\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from js2py->pipwin) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal>=1.2->js2py->pipwin) (2022.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pipwin) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pipwin) (3.0.4)\n",
            "Building wheels for collected packages: pipwin, docopt, pyjsparser\n",
            "  Building wheel for pipwin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipwin: filename=pipwin-0.5.2-py2.py3-none-any.whl size=8791 sha256=ec50d7ab82db771bd0db7bc8664df5cabb438590a6ee08d00595b54123c4e4b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/df/15/51b9c5d152e27b7fad998993eba50e15dfa709cc7438557f7e\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=49a8badf9bb089f7cf5a98ca9ba3d2b71b3edc5b8c98e561ba1ca916f91b2a70\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=26000 sha256=42bcccdc05aa45b3b96bacf4777e4b789fa94af645f0ae8c82cc2cb6a48d3ff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/80/ac/dcd2bdbd03dd2b7b7e2bf3e5afbda6a1ab7935bbce314969da\n",
            "Successfully built pipwin docopt pyjsparser\n",
            "Installing collected packages: soupsieve, pyjsparser, pySmartDL, pyprind, js2py, docopt, beautifulsoup4, pipwin\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 docopt-0.6.2 js2py-0.71 pipwin-0.5.2 pySmartDL-1.3.4 pyjsparser-2.7.1 pyprind-2.11.3 soupsieve-2.3.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(flag==True):\n",
        "    with sr.Microphone() as source:\n",
        "        audio= r.listen(source)\n",
        "    try:\n",
        "        user_response = format(r.recognize(audio))\n",
        "        print(\"\\033[91m {}\\033[00m\" .format(\"YOU SAID : \"+user_response))\n",
        "    except sr.UnknownValueError:\n",
        "        prYellow(\"Oops! Didn't catch that\")\n",
        "        pass\n",
        "    \n",
        "    #user_response = input()\n",
        "    #user_response=user_response.lower()\n",
        "    clas=classifier.classify(dialogue_act_features(user_response))\n",
        "    if(clas!='Bye'):\n",
        "        if(clas=='Emotion'):\n",
        "            flag=False\n",
        "            prYellow(\"Jarvis: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"\\033[93m {}\\033[00m\" .format(\"Jarvis: \"+greeting(user_response)))\n",
        "            else:\n",
        "                print(\"\\033[93m {}\\033[00m\" .format(\"Jarvis: \",end=\"\"))\n",
        "                res=(response(user_response))\n",
        "                prYellow(res)\n",
        "                sent_tokens.remove(user_response)\n",
        "                tts = gTTS(res, 'en')\n",
        "                tts.save(file)\n",
        "                os.system(\"mpg123 \" + file)\n",
        "    else:\n",
        "        flag=False\n",
        "        prYellow(\"Jarvis: Bye! take care..\")"
      ],
      "metadata": {
        "id": "H_8xjuzwU_jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cs2QyA1kV3Q-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}